{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import unix_timestamp, current_timestamp, col, lit, split, datediff, floor, current_date, to_date, when, mean, expr, udf\n",
    "from pyspark.sql.types import LongType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('Lab3') \\\n",
    "    .config('spark.sql.legacy.timeParserPolicy', 'LEGACY') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark server is accessible here (click on Spark UI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('PYSPARK_PYTHON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display head of spark dataframe in a way that looks nice in notebook\n",
    "def display_head(df: pyspark.sql.DataFrame, n: int):\n",
    "    return df.limit(n).toPandas().head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "### Names.csv\n",
    "- Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "- Dodaj kolumnę w której wyliczysz wzrost w stopach (feet)\n",
    "- Odpowiedz na pytanie jakie jest najpopularniesze imię?\n",
    "- Dodaj kolumnę i policz wiek aktorów\n",
    "- Usuń kolumny (bio, death_details)\n",
    "- Zmień nazwy kolumn - dodaj kapitalizaję i usuń _\n",
    "- Posortuj dataframe po i imeniu rosnąco\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "names_df = spark.read.csv('../data/names.csv', header=True, inferSchema=True)\n",
    "display_head(names_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res_df = names_df \\\n",
    "    .withColumn(\"execution_time\", unix_timestamp(current_timestamp())) \\\n",
    "    .withColumn(\"height_feet\", col('height') * lit(3.28)) \\\n",
    "\n",
    "display_head(res_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "most_popular_names = names_df \\\n",
    "    .withColumn(\"first_name\", split(col('name'), ' ').getItem(0)) \\\n",
    "    .groupBy('first_name').count() \\\n",
    "    .sort('count', ascending=False)\n",
    "\n",
    "most_popular_names.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "most_popular_name = most_popular_names.first()\n",
    "print(f'Most popular name is {most_popular_name[\"first_name\"]} with {most_popular_name[\"count\"]} occurances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res_df = res_df \\\n",
    "    .withColumn('birth_date1', to_date(col('date_of_birth'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn('birth_date2', to_date(col('date_of_birth'), 'dd.MM.yyyy')) \\\n",
    "    .withColumn('date_of_birth', when(col('birth_date1').isNotNull(), col('birth_date1')).otherwise(col('birth_date2'))) \\\n",
    "    .drop('birth_date1', 'birth_date2') \\\n",
    "    .withColumn('is_dead', when(col('date_of_death').isNotNull() | col('place_of_death').isNotNull() | col('reason_of_death').isNotNull() | col('death_details').isNotNull(), 1).otherwise(0)) \\\n",
    "    .withColumn('death_date1', to_date(col('date_of_death'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn('death_date2', to_date(col('date_of_death'), 'dd.MM.yyyy')) \\\n",
    "    .withColumn('date_of_death', when(col('death_date1').isNotNull(), col('death_date1')).otherwise(col('death_date2'))) \\\n",
    "    .drop('death_date1', 'death_date2') \\\n",
    "    .withColumn('date_of_death', to_date(col('date_of_death'), 'dd.MM.yyyy')) \\\n",
    "    .withColumn('age', when(col('is_dead') == 1, floor(datediff(col('date_of_death'), col('date_of_birth')) / 365.25))\n",
    "               .otherwise(floor(datediff(current_date(), col('date_of_birth')) / 365.25)) )\n",
    "\n",
    "res_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def to_camel_case(s: str):\n",
    "    return ''.join(word.capitalize() for word in s.split('_'))\n",
    "\n",
    "res_df = res_df.drop('bio', 'death_details')\n",
    "\n",
    "for col_name in res_df.columns:\n",
    "    res_df = res_df.withColumnRenamed(col_name, to_camel_case(col_name))\n",
    "\n",
    "res_df = res_df.sort('Name', ascending=True)\n",
    "\n",
    "display_head(res_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Movies.csv\n",
    "\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dodaj kolumnę która wylicza ile lat upłynęło od publikacji filmu\n",
    "* Dodaj kolumnę która pokaże budżet filmu jako wartość numeryczną, (trzeba usunac znaki walut)\n",
    "* Usuń wiersze z dataframe gdzie wartości są null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_df = spark.read.csv('../data/movies.csv', header=True, inferSchema=True)\n",
    "\n",
    "display_head(movies_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res_df = movies_df \\\n",
    "    .withColumn(\"execution_time\", unix_timestamp(current_timestamp())) \\\n",
    "    .withColumn('pub_date1', to_date(col('date_published'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn('pub_date2', to_date(col('date_published'), 'dd.MM.yyyy')) \\\n",
    "    .withColumn('pub_date3', to_date(col('date_published'), 'yyyy')) \\\n",
    "    .withColumn('date_published', when(col('pub_date1').isNotNull(), col('pub_date1')).otherwise(col('pub_date2'))) \\\n",
    "    .withColumn('date_published', when(col('date_published').isNotNull(), col('date_published')).otherwise(col('pub_date3'))) \\\n",
    "    .drop('pub_date1', 'pub_date2', 'pub_date3') \\\n",
    "    .withColumn('years_from_published', floor(datediff(current_date(), col('date_published')) / 365.25)) \\\n",
    "    .withColumn('budget_numeric', split(col('budget'), ' ').getItem(1)) \\\n",
    "    .dropna()\n",
    "\n",
    "display_head(res_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "### Ratings.csv\n",
    "\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dla każdego z poniższych wyliczeń nie bierz pod uwagę `nulls`\n",
    "* Dodaj nowe kolumny i policz mean i median dla wartości głosów (1 d 10)\n",
    "* Dla każdej wartości mean i median policz jaka jest różnica między weighted_average_vote\n",
    "* Kto daje lepsze oceny chłopcy czy dziewczyny dla całego setu\n",
    "* Dla jednej z kolumn zmień typ danych do `long`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ratings_df = spark.read.csv('../data/ratings.csv', header=True, inferSchema=True)\n",
    "\n",
    "display_head(ratings_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res_df = ratings_df \\\n",
    "    .withColumn(\"execution_time\", unix_timestamp(current_timestamp())) \\\n",
    "    .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "col_names = [f'votes_{i+1}' for i in range(10)]\n",
    "mean_expr = expr(f'({\" + \".join(col_names)}) / 10')\n",
    "\n",
    "def median(*values):\n",
    "    return float(np.median(values))\n",
    "\n",
    "median_udf=udf(median, DoubleType())\n",
    "res_df = res_df \\\n",
    "    .withColumn('votes1-10_mean', mean_expr) \\\n",
    "    .withColumn('votes1-10median', median_udf(*col_names)) \\\n",
    "    .withColumn('diff_wa_mean1-10', col('weighted_average_vote') - col('votes1-10_mean')) \\\n",
    "    .withColumn('diff_wa_median1-10', col('weighted_average_vote') - col('votes1-10median'))\n",
    "\n",
    "display_head(res_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "female_votes_larger = ratings_df.filter(col('females_allages_avg_vote') > col('males_allages_avg_vote')).count()\n",
    "\n",
    "male_votes_larger = ratings_df.filter(col('females_allages_avg_vote') < col('males_allages_avg_vote')).count()\n",
    "\n",
    "votes_equal = ratings_df.filter(col('females_allages_avg_vote') == col('males_allages_avg_vote')).count()\n",
    "\n",
    "mean_votes = ratings_df.agg(mean(col(\"females_allages_avg_vote\")).alias(\"female_avg_vote\"), mean(col(\"males_allages_avg_vote\")).alias(\"male_avg_vote\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "female_avg_vote = mean_votes[0]['female_avg_vote']\n",
    "male_avg_vote = mean_votes[0]['male_avg_vote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of male larger average votes: {male_votes_larger}')\n",
    "print(f'Number of female larger average votes: {female_votes_larger}')\n",
    "print(f'Number of equal average votes: {votes_equal}')\n",
    "print(f'Male average vote: {male_avg_vote}')\n",
    "print(f'Female average vote: {female_avg_vote}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can conclude that female's votes are more favorable that male one's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ratings_df.withColumn('total_votes', col('total_votes').cast(LongType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "- In jobs section we can see state of executors and jobs suceeded/failed/running\n",
    "- In stages we can see stages of execution of each job\n",
    "- In Storage section we can see RDD and cache's\n",
    "- In Environment we can see configuration of spark environment\n",
    "- In executors we get overview of attached executors\n",
    "- In SQL/DaataFrame we can see executed queries with their times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-env",
   "language": "python",
   "name": "big-data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
